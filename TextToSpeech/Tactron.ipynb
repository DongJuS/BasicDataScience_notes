{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a98082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "경량형 온디바이스 한국어 TTS 모델 (Tacotron2 기반)\n",
    "JSON 파일의 TransLabelText를 활용한 텍스트 처리 및 학습\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "from jamo import h2j, j2hcj\n",
    "import pickle\n",
    "\n",
    "# 한국어 텍스트 전처리 클래스\n",
    "class KoreanTextProcessor:\n",
    "    def __init__(self):\n",
    "        # 한국어 자모 분리를 위한 매핑\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self._build_vocab()\n",
    "    \n",
    "    # KoreanTextProcessor.__init__ 수정\n",
    "    def _build_vocab(self):\n",
    "        chars = ['<PAD>', '<START>', '<END>', ' ', '!', '?', '.', ',', ';', ':', '-', '(', ')']\n",
    "        cho = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', \n",
    "            'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "        jung = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ',\n",
    "                'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "        jong = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ',\n",
    "                'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',\n",
    "                'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "        \n",
    "        all_chars = chars + cho + jung + jong\n",
    "        \n",
    "        for i, char in enumerate(all_chars):\n",
    "            self.char_to_id[char] = i\n",
    "            self.id_to_char[i] = char\n",
    "        \n",
    "        print(f\"어휘 크기: {len(all_chars)}\")\n",
    "    \n",
    "    def korean_to_jamo(self, text: str) -> str:\n",
    "        \"\"\"한글을 자모로 분리\"\"\"\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if '가' <= char <= '힣':  # 한글인 경우\n",
    "                decomposed = j2hcj(h2j(char))\n",
    "                result.append(decomposed)\n",
    "            else:\n",
    "                result.append(char)\n",
    "        return ''.join(result)\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"텍스트 정규화\"\"\"\n",
    "        # 특수문자 정리\n",
    "        text = re.sub(r'[^\\w\\s가-힣!?.,;:]', '', text)\n",
    "        # 공백 정리\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def text_to_sequence(self, text: str) -> List[int]:\n",
    "        text = self.normalize_text(text)\n",
    "        text = self.korean_to_jamo(text)\n",
    "        \n",
    "        sequence = [1]  # START 토큰 (<START> = 1)\n",
    "        max_vocab_id = len(self.char_to_id) - 1\n",
    "        \n",
    "        for char in text:\n",
    "            char_id = self.char_to_id.get(char, 0)  # PAD = 0\n",
    "            char_id = min(char_id, max_vocab_id)\n",
    "            sequence.append(char_id)\n",
    "        \n",
    "        sequence.append(2)  # END 토큰 (<END> = 2)\n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence: List[int]) -> str:\n",
    "        \"\"\"시퀀스를 텍스트로 변환\"\"\"\n",
    "        text = []\n",
    "        for id in sequence:\n",
    "            if id in self.id_to_char:\n",
    "                char = self.id_to_char[id]\n",
    "                if char not in ['<PAD>', '<START>', '<END>']:\n",
    "                    text.append(char)\n",
    "        return ''.join(text)\n",
    "\n",
    "# JSON 데이터셋 클래스\n",
    "class KoreanTTSDataset(Dataset):\n",
    "    def __init__(self, json_dir: str, audio_dir: str, text_processor: KoreanTextProcessor):\n",
    "        self.text_processor = text_processor\n",
    "        self.audio_dir = audio_dir\n",
    "        self.data = []\n",
    "        \n",
    "        # 디렉토리에서 모든 JSON 파일 찾기\n",
    "        json_files = self._find_json_files(json_dir)\n",
    "        print(f\"찾은 JSON 파일 수: {len(json_files):,}\")\n",
    "        \n",
    "        # JSON 파일들에서 데이터 로드 (진행률 표시)\n",
    "        print(\"🔄 JSON 파일 처리 중...\")\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        for i, json_file in enumerate(json_files):\n",
    "            self._load_json_data(json_file)\n",
    "            \n",
    "           # 진행률 표시 (500개마다 또는 5%마다) - 원래 1000개에서 500개로 변경\n",
    "            if (i + 1) % 500 == 0 or (i + 1) % max(1, total_files // 20) == 0:\n",
    "                progress = (i + 1) / total_files * 100\n",
    "                print(f\"진행률: {i+1:,}/{total_files:,} ({progress:.1f}%) - 유효 데이터: {len(self.data):,}개\")\n",
    "        \n",
    "        print(f\"✅ JSON 파일 처리 완료: 총 {len(self.data):,}개 데이터 로드\")\n",
    "    \n",
    "    def _find_json_files(self, json_dir: str) -> List[str]:\n",
    "        \"\"\"디렉토리에서 모든 JSON 파일을 재귀적으로 찾기\"\"\"\n",
    "        json_files = []\n",
    "        \n",
    "        if not os.path.exists(json_dir):\n",
    "            print(f\"경고: 디렉토리가 존재하지 않습니다: {json_dir}\")\n",
    "            return json_files\n",
    "        \n",
    "        print(\"📂 JSON 파일 스캔 중...\")\n",
    "        \n",
    "        # os.walk()를 사용하여 하위 디렉토리까지 모든 JSON 파일 찾기\n",
    "        for root, dirs, files in os.walk(json_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.json'):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    json_files.append(full_path)\n",
    "                    if len(json_files) % 1000 == 0:  # 1000개마다 진행상황 표시\n",
    "                        print(f\"📄 JSON 파일 스캔 중... {len(json_files):,}개 발견\")\n",
    "        \n",
    "        print(f\"✅ JSON 파일 스캔 완료: {len(json_files):,}개\")\n",
    "        return sorted(json_files)\n",
    "    \n",
    "    def _load_json_data(self, json_file: str):\n",
    "        \"\"\"JSON 파일에서 TransLabelText 데이터 로드 (다양한 구조 지원)\"\"\"\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            trans_label_text = None\n",
    "            \n",
    "            # \"전사정보\" 키에서 TransLabelText 찾기 (우선순위 1)\n",
    "            if isinstance(data, dict) and '전사정보' in data:\n",
    "                if isinstance(data['전사정보'], dict) and 'TransLabelText' in data['전사정보']:\n",
    "                    trans_label_text = data['전사정보']['TransLabelText']\n",
    "                    \n",
    "            # 만약 위에서 못 찾았다면 다른 구조도 확인\n",
    "            if not trans_label_text:\n",
    "                if isinstance(data, dict):\n",
    "                    # 직접 TransLabelText가 있는 경우\n",
    "                    if 'TransLabelText' in data:\n",
    "                        trans_label_text = data['TransLabelText']\n",
    "                    else:\n",
    "                        # 재귀적으로 찾기\n",
    "                        trans_label_text = self._find_trans_label_text_recursive(data)\n",
    "                elif isinstance(data, list):\n",
    "                    # 리스트 형태인 경우\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict) and '전사정보' in item:\n",
    "                            if isinstance(item['전사정보'], dict) and 'TransLabelText' in item['전사정보']:\n",
    "                                trans_label_text = item['전사정보']['TransLabelText']\n",
    "                                break\n",
    "            \n",
    "            # 오디오 파일 경로 찾기 (JSON 파일명 기반)\n",
    "            if trans_label_text and isinstance(trans_label_text, str) and trans_label_text.strip():\n",
    "                audio_path = self._find_audio_path_improved(json_file)\n",
    "                \n",
    "                self.data.append({\n",
    "                    'text': trans_label_text.strip(),\n",
    "                    'audio': audio_path,\n",
    "                    'json_file': json_file\n",
    "                })\n",
    "                \n",
    "                # 처음 5개 파일만 상태 출력\n",
    "                if len(self.data) <= 5:\n",
    "                    if audio_path and os.path.exists(audio_path):\n",
    "                        print(f\"✅ 매칭 성공: {os.path.basename(json_file)} -> {os.path.basename(audio_path)}\")\n",
    "                    else:\n",
    "                        print(f\"❌ 오디오 없음: {os.path.basename(json_file)} -> {audio_path}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            if len(self.data) < 3:  # 처음 3개만 오류 출력\n",
    "                print(f\"JSON 파일 로딩 오류 {os.path.basename(json_file)}: {e}\")\n",
    "    \n",
    "    def _find_trans_label_text_recursive(self, data, max_depth=2, current_depth=0):\n",
    "        \"\"\"재귀적으로 TransLabelText 찾기 (깊이 제한)\"\"\"\n",
    "        if current_depth >= max_depth:\n",
    "            return None\n",
    "            \n",
    "        if isinstance(data, dict):\n",
    "            # 우선 \"전사정보\" 키 확인\n",
    "            if '전사정보' in data and isinstance(data['전사정보'], dict):\n",
    "                if 'TransLabelText' in data['전사정보']:\n",
    "                    return data['전사정보']['TransLabelText']\n",
    "            \n",
    "            # 직접 TransLabelText 키가 있는지 확인\n",
    "            if 'TransLabelText' in data:\n",
    "                return data['TransLabelText']\n",
    "            \n",
    "            # 다른 키들을 재귀적으로 탐색 (깊이 제한)\n",
    "            for key, value in data.items():\n",
    "                if key != '기본정보' and isinstance(value, (dict, list)):  # 기본정보는 제외\n",
    "                    result = self._find_trans_label_text_recursive(value, max_depth, current_depth + 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        \n",
    "        elif isinstance(data, list):\n",
    "            for item in data[:3]:  # 리스트의 처음 3개만 확인\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    result = self._find_trans_label_text_recursive(item, max_depth, current_depth + 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _find_audio_path_improved(self, json_file: str):\n",
    "        \"\"\"개선된 오디오 파일 경로 찾기\"\"\"\n",
    "        # JSON 파일명에서 확장자 제거\n",
    "        base_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "        json_dir = os.path.dirname(json_file)\n",
    "        \n",
    "        # 1. JSON 파일과 같은 디렉토리에서 .wav 파일 찾기\n",
    "        wav_path = os.path.join(json_dir, base_name + '.wav')\n",
    "        if os.path.exists(wav_path):\n",
    "            return wav_path\n",
    "        \n",
    "        # 2. 라벨링데이터 -> 원천데이터로 경로 변경\n",
    "        if '라벨링데이터' in json_dir:\n",
    "            audio_dir = json_dir.replace('라벨링데이터', '원천데이터')\n",
    "            # TL22 -> TS22 변경\n",
    "            if 'TL22' in audio_dir:\n",
    "                audio_dir = audio_dir.replace('TL22', 'TS22')\n",
    "            \n",
    "            wav_path = os.path.join(audio_dir, base_name + '.wav')\n",
    "            if os.path.exists(wav_path):\n",
    "                return wav_path\n",
    "        \n",
    "        # 3. 직접 지정된 audio_dir 사용 (만약 설정되어 있다면)\n",
    "        if hasattr(self, 'audio_dir') and self.audio_dir:\n",
    "            # JSON의 상대 경로 구조를 audio_dir에 적용\n",
    "            json_relative_parts = json_dir.split(os.sep)\n",
    "            # 0001_G2A2E7_KMJ 같은 폴더명 찾기\n",
    "            speaker_folder = None\n",
    "            for part in json_relative_parts:\n",
    "                if '_G2A2E7_' in part:  # 화자 폴더 패턴\n",
    "                    speaker_folder = part\n",
    "                    break\n",
    "            \n",
    "            if speaker_folder:\n",
    "                wav_path = os.path.join(self.audio_dir, speaker_folder, base_name + '.wav')\n",
    "                if os.path.exists(wav_path):\n",
    "                    return wav_path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        text = item['text']\n",
    "        text_sequence = self.text_processor.text_to_sequence(text)\n",
    "        \n",
    "        # 오디오 경로가 이미 전체 경로임\n",
    "        mel_spectrogram = self._process_audio(item['audio'])  # self.audio_dir 제거\n",
    "        \n",
    "        return {\n",
    "            'text': torch.LongTensor(text_sequence),\n",
    "            'mel': torch.FloatTensor(mel_spectrogram),\n",
    "            'text_length': len(text_sequence),\n",
    "            'mel_length': mel_spectrogram.shape[1]\n",
    "        }\n",
    "    \n",
    "    def _process_audio(self, audio_path: str) -> np.ndarray:\n",
    "        \"\"\"오디오 파일을 멜 스펙트로그램으로 변환\"\"\"\n",
    "        try:\n",
    "            # 오디오 로드\n",
    "            audio, sr = librosa.load(audio_path, sr=22050)\n",
    "            \n",
    "            # 멜 스펙트로그램 생성\n",
    "            mel = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=sr,\n",
    "                n_fft=1024,\n",
    "                hop_length=256,\n",
    "                win_length=1024,\n",
    "                n_mels=80,\n",
    "                fmin=0,\n",
    "                fmax=8000\n",
    "            )\n",
    "            \n",
    "            # 로그 스케일 변환\n",
    "            mel = np.log(mel + 1e-9)\n",
    "            \n",
    "            return mel\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오디오 처리 오류 {audio_path}: {e}\")\n",
    "            # 더미 데이터 반환\n",
    "            return np.zeros((80, 100))\n",
    "\n",
    "# Tacotron2 모델 구성요소들\n",
    "class LocationLayer(nn.Module):\n",
    "    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):\n",
    "        super(LocationLayer, self).__init__()\n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.location_conv = nn.Conv1d(2, attention_n_filters, kernel_size=attention_kernel_size, padding=padding, bias=False)\n",
    "        self.location_dense = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "    \n",
    "    def forward(self, attention_weights_cat):\n",
    "        processed_attention = self.location_conv(attention_weights_cat)\n",
    "        processed_attention = processed_attention.transpose(1, 2)\n",
    "        processed_attention = self.location_dense(processed_attention)\n",
    "        return processed_attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = nn.Linear(attention_rnn_dim, attention_dim, bias=False)\n",
    "        self.memory_layer = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters, attention_location_kernel_size, attention_dim)\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "    \n",
    "    def get_alignment_energies(self, query, processed_memory, attention_weights_cat):\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        energies = self.v(torch.tanh(processed_query + processed_attention_weights + processed_memory))\n",
    "        energies = energies.squeeze(-1)\n",
    "        return energies\n",
    "    \n",
    "    def forward(self, attention_hidden_state, memory, processed_memory, attention_weights_cat, mask):\n",
    "        alignment = self.get_alignment_energies(attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "        \n",
    "        if mask is not None:\n",
    "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "        \n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "        \n",
    "        return attention_context, attention_weights\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes):\n",
    "        super(Prenet, self).__init__()\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_size, out_size) for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "        return x\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "    def __init__(self, mel_dim, postnet_embedding_dim, postnet_kernel_size, postnet_n_convolutions):\n",
    "        super(Postnet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        \n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(mel_dim, postnet_embedding_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_embedding_dim, postnet_embedding_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(postnet_embedding_dim, mel_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(mel_dim)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
    "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_n_convolutions, encoder_embedding_dim, encoder_kernel_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        std = np.sqrt(2.0 / (vocab_size + embedding_dim))\n",
    "        val = np.sqrt(3.0) * std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        \n",
    "        convolutions = []\n",
    "        for _ in range(encoder_n_convolutions):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(embedding_dim, encoder_embedding_dim, kernel_size=encoder_kernel_size, stride=1, padding=int((encoder_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(encoder_embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "            convolutions.append(conv_layer)\n",
    "            embedding_dim = encoder_embedding_dim\n",
    "        \n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        self.lstm = nn.LSTM(encoder_embedding_dim, int(encoder_embedding_dim // 2), 1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        input_lengths = input_lengths.cpu().numpy()\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def inference(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, mel_dim, encoder_embedding_dim, attention_rnn_dim, decoder_rnn_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size, prenet_dim, max_decoder_steps, gate_threshold, p_attention_dropout, p_decoder_dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mel_dim = mel_dim\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.attention_rnn_dim = attention_rnn_dim\n",
    "        self.decoder_rnn_dim = decoder_rnn_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_location_n_filters = attention_location_n_filters\n",
    "        self.attention_location_kernel_size = attention_location_kernel_size\n",
    "        self.prenet_dim = prenet_dim\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.gate_threshold = gate_threshold\n",
    "        self.p_attention_dropout = p_attention_dropout\n",
    "        self.p_decoder_dropout = p_decoder_dropout\n",
    "        \n",
    "        self.prenet = Prenet(mel_dim, [prenet_dim, prenet_dim])\n",
    "        \n",
    "        self.attention_rnn = nn.LSTMCell(prenet_dim + encoder_embedding_dim, attention_rnn_dim)\n",
    "        \n",
    "        self.attention_layer = Attention(attention_rnn_dim, encoder_embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size)\n",
    "        \n",
    "        self.decoder_rnn = nn.LSTMCell(attention_rnn_dim + encoder_embedding_dim, decoder_rnn_dim, 1)\n",
    "        \n",
    "        self.linear_projection = nn.Linear(decoder_rnn_dim + encoder_embedding_dim, mel_dim)\n",
    "        \n",
    "        self.gate_layer = nn.Linear(decoder_rnn_dim + encoder_embedding_dim, 1, bias=True)\n",
    "    \n",
    "    def get_go_frame(self, memory):\n",
    "        B = memory.size(0)\n",
    "        go_frame = torch.zeros(B, self.mel_dim, device=memory.device, dtype=memory.dtype)\n",
    "        return go_frame\n",
    "    \n",
    "    # def initialize_decoder_states(self, memory, mask):\n",
    "    #     B = memory.size(0)\n",
    "    #     MAX_TIME = memory.size(1)\n",
    "        \n",
    "    #     attention_hidden = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_cell = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     decoder_hidden = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "    #     decoder_cell = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     attention_weights = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_weights_cum = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_context = torch.zeros(B, self.encoder_embedding_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     return (attention_hidden, attention_cell, decoder_hidden, decoder_cell, attention_weights, attention_weights_cum, attention_context)\n",
    "    def initialize_decoder_states(self, memory):\n",
    "        \"\"\"디코더 상태 초기화\"\"\"\n",
    "        B = memory.size(0)\n",
    "        MAX_TIME = memory.size(1)\n",
    "        \n",
    "        self.attention_hidden = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_cell = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.decoder_hidden = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        self.decoder_cell = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.attention_weights = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_weights_cum = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_context = torch.zeros(B, self.encoder_embedding_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "        self.mask = None\n",
    "    \n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        decoder_inputs = decoder_inputs.view(decoder_inputs.size(0), int(decoder_inputs.size(1) / self.mel_dim), self.mel_dim)\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        return decoder_inputs\n",
    "    \n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "        gate_outputs = gate_outputs.contiguous()\n",
    "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "        mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.mel_dim)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "    \n",
    "    def decode(self, decoder_input):\n",
    "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "        \n",
    "        self.attention_hidden, self.attention_cell = self.attention_rnn(cell_input, (self.attention_hidden, self.attention_cell))\n",
    "        self.attention_hidden = F.dropout(self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "        \n",
    "        attention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "        self.attention_context, self.attention_weights = self.attention_layer(self.attention_hidden, self.memory, self.processed_memory, attention_weights_cat, self.mask)\n",
    "        \n",
    "        self.attention_weights_cum += self.attention_weights\n",
    "        decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "        \n",
    "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "        self.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "        \n",
    "        decoder_hidden_attention_context = torch.cat((self.decoder_hidden, self.attention_context), dim=1)\n",
    "        decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "        \n",
    "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "        \n",
    "        return decoder_output, gate_prediction, self.attention_weights\n",
    "    \n",
    "    # 기존 코드\n",
    "    # def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "    #     decoder_input = self.get_go_frame(memory).unsqueeze(1)\n",
    "    #     decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "    #     decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=1)\n",
    "    #     decoder_inputs = self.prenet(decoder_inputs)\n",
    "        \n",
    "    #     self.initialize_decoder_states(memory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "        \n",
    "    #     mel_outputs, gate_outputs, alignments = [], [], []\n",
    "    #     while len(mel_outputs) < decoder_inputs.size(1) - 1:\n",
    "    #         decoder_input = decoder_inputs[:, len(mel_outputs)]\n",
    "    #         mel_output, gate_output, attention_weights = self.decode(decoder_input)\n",
    "    #         mel_outputs += [mel_output.squeeze(1)]\n",
    "    #         gate_outputs += [gate_output.squeeze(1)]\n",
    "    #         alignments += [attention_weights]\n",
    "        \n",
    "    #     mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(mel_outputs, gate_outputs, alignments)\n",
    "        \n",
    "    #     return mel_outputs, gate_outputs, alignments\n",
    "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "        # 간단한 더미 출력으로 테스트\n",
    "        batch_size = memory.size(0)\n",
    "        max_mel_len = decoder_inputs.size(1) // self.mel_dim\n",
    "        \n",
    "        # 더미 출력 생성\n",
    "        mel_outputs = torch.randn(batch_size, self.mel_dim, max_mel_len, device=memory.device)\n",
    "        gate_outputs = torch.zeros(batch_size, max_mel_len, device=memory.device)\n",
    "        alignments = torch.zeros(batch_size, max_mel_len, memory.size(1), device=memory.device)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "# Tacotron2 메인 모델\n",
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        \n",
    "        # 모델 하이퍼파라미터 (경량화를 위해 크기 축소)\n",
    "        self.embedding_dim = 256  # 원래 512에서 축소\n",
    "        self.encoder_embedding_dim = 256  # 원래 512에서 축소\n",
    "        self.encoder_n_convolutions = 3\n",
    "        self.encoder_kernel_size = 5\n",
    "        self.attention_rnn_dim = 512  # 원래 1024에서 축소\n",
    "        self.attention_dim = 64  # 원래 128에서 축소\n",
    "        self.attention_location_n_filters = 16  # 원래 32에서 축소\n",
    "        self.attention_location_kernel_size = 31\n",
    "        self.decoder_rnn_dim = 512  # 원래 1024에서 축소\n",
    "        self.prenet_dim = 128  # 원래 256에서 축소\n",
    "        self.max_decoder_steps = 1000\n",
    "        self.gate_threshold = 0.5\n",
    "        self.p_attention_dropout = 0.1\n",
    "        self.p_decoder_dropout = 0.1\n",
    "        self.postnet_embedding_dim = 256  # 원래 512에서 축소\n",
    "        self.postnet_kernel_size = 5\n",
    "        self.postnet_n_convolutions = 5\n",
    "        self.mel_dim = 80\n",
    "        safe_vocab_size = max(vocab_size, 100)\n",
    "        \n",
    "        # self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.embedding = nn.Embedding(safe_vocab_size, self.embedding_dim)\n",
    "        std = np.sqrt(2.0 / (safe_vocab_size + self.embedding_dim))\n",
    "        val = np.sqrt(3.0) * std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        \n",
    "        self.encoder = Encoder(safe_vocab_size, self.embedding_dim, self.encoder_n_convolutions, self.encoder_embedding_dim, self.encoder_kernel_size)\n",
    "        \n",
    "        self.decoder = Decoder(self.mel_dim, self.encoder_embedding_dim, self.attention_rnn_dim, self.decoder_rnn_dim, self.attention_dim, self.attention_location_n_filters, self.attention_location_kernel_size, self.prenet_dim, self.max_decoder_steps, self.gate_threshold, self.p_attention_dropout, self.p_decoder_dropout)\n",
    "        \n",
    "        self.postnet = Postnet(self.mel_dim, self.postnet_embedding_dim, self.postnet_kernel_size, self.postnet_n_convolutions)\n",
    "    \n",
    "    def forward(self, text_inputs, text_lengths, mels, mel_lengths):\n",
    "        text_lengths, mel_lengths = text_lengths.data, mel_lengths.data\n",
    "        \n",
    "        # 이 라인을 삭제하거나 주석 처리하세요.\n",
    "        # embedded_inputs = self.embedding(text_inputs).transpose(1, 2) \n",
    "        \n",
    "        # text_inputs를 encoder에 직접 전달합니다.\n",
    "        encoder_outputs = self.encoder(text_inputs, text_lengths)\n",
    "        \n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(encoder_outputs, mels, text_lengths)\n",
    "        \n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
    "\n",
    "\n",
    "# def get_mask_from_lengths(lengths):\n",
    "#     max_len = torch.max(lengths).item()\n",
    "#     ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
    "#     mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "#     return mask\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device)  # device 명시\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask\n",
    "\n",
    "# collate_fn 시그니처 수정 및 로직 변경\n",
    "def collate_fn(batch, vocab_size): # vocab_size를 인자로 받도록 수정\n",
    "    batch.sort(key=lambda x: x['text_length'], reverse=True)\n",
    "    \n",
    "    texts = [item['text'] for item in batch]\n",
    "    mels = [item['mel'] for item in batch]\n",
    "    text_lengths = torch.LongTensor([len(item['text']) for item in batch])\n",
    "    mel_lengths = torch.LongTensor([item['mel'].shape[1] for item in batch])\n",
    "    \n",
    "    # 인덱스 검증 (동적으로 vocab_size 사용)\n",
    "    max_valid_index = vocab_size - 1\n",
    "    for i, text in enumerate(texts):\n",
    "        if text.numel() > 0 and text.max() > max_valid_index:\n",
    "            print(f\"경고: 텍스트 {i}에서 범위 초과 인덱스({text.max()})가 어휘사전 크기({vocab_size})를 벗어났습니다. PAD(0)으로 변경합니다.\")\n",
    "            text[text > max_valid_index] = 0  # 범위를 벗어나는 값을 PAD 토큰(0)으로 강제 변환\n",
    "            texts[i] = text\n",
    "\n",
    "    # 파이토치 내장 함수를 사용한 효율적인 패딩\n",
    "    text_padded = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 멜 스펙트로그램 패딩\n",
    "    mel_padded = torch.zeros(len(batch), 80, mel_lengths.max())\n",
    "    for i, mel in enumerate(mels):\n",
    "        mel_padded[i, :, :mel.shape[1]] = mel\n",
    "    \n",
    "    return text_padded, mel_padded, text_lengths, mel_lengths\n",
    "\n",
    "\n",
    "# 학습 함수\n",
    "def train_tacotron2(json_dir, audio_dir, save_dir, epochs=100, batch_size=8, lr=1e-3):\n",
    "    \"\"\"Tacotron2 모델 학습 (디렉토리 기반)\"\"\"\n",
    "    from functools import partial\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"사용 디바이스: {device}\")\n",
    "    \n",
    "    # 텍스트 프로세서 초기화\n",
    "    text_processor = KoreanTextProcessor()\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    \n",
    "    # 데이터셋 및 데이터로더 생성\n",
    "    print(f\"JSON 파일 디렉토리: {json_dir}\")\n",
    "    print(f\"오디오 파일 디렉토리: {audio_dir}\")\n",
    "    \n",
    "    dataset = KoreanTTSDataset(json_dir, audio_dir, text_processor)\n",
    "    dataset.data = dataset.data[:100]  # 처음 10,000개만 사용\n",
    "\n",
    "    collate_with_vocab = partial(collate_fn, vocab_size=vocab_size)\n",
    "\n",
    "    \n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                        collate_fn=collate_with_vocab, num_workers=0)\n",
    "    \n",
    "    print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "    print(f\"vocabulary 크기: {vocab_size}\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"❌ 오류: 데이터셋이 비어있습니다. JSON 파일과 오디오 파일 경로를 확인해주세요.\")\n",
    "        return None, None\n",
    "    \n",
    "    # 데이터 샘플 확인\n",
    "    print(\"\\n=== 데이터 샘플 확인 ===\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        sample = dataset.data[i]\n",
    "        print(f\"샘플 {i+1}:\")\n",
    "        print(f\"  텍스트: {sample['text'][:50]}...\")\n",
    "        print(f\"  오디오: {sample['audio']}\")\n",
    "        print(f\"  JSON: {sample['json_file']}\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = Tacotron2(vocab_size).to(device)\n",
    "    \n",
    "    # 옵티마이저\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    \n",
    "    # 손실 함수\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 학습 루프\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_batches = len(dataloader)\n",
    "        for batch_idx, (text_padded, mel_padded, text_lengths, mel_lengths) in enumerate(dataloader):\n",
    "            text_padded = text_padded.to(device)\n",
    "            mel_padded = mel_padded.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            mel_lengths = mel_lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(\n",
    "                text_padded, text_lengths, mel_padded, mel_lengths\n",
    "            )\n",
    "            \n",
    "            # 손실 계산\n",
    "            mel_loss = criterion(mel_outputs, mel_padded)\n",
    "            mel_postnet_loss = criterion(mel_outputs_postnet, mel_padded)\n",
    "            gate_loss = nn.BCEWithLogitsLoss()(gate_outputs, torch.zeros_like(gate_outputs))\n",
    "            \n",
    "            total_batch_loss = mel_loss + mel_postnet_loss + gate_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_batch_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            \n",
    "            if batch_idx % max(1, total_batches // 10) == 0 or (batch_idx + 1) % 5 == 0:\n",
    "                progress = (batch_idx + 1) / total_batches * 100\n",
    "                print(f\"📊 Batch {batch_idx+1:3d}/{total_batches} ({progress:5.1f}%) - Loss: {total_batch_loss.item():.4f}\")\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"✅ Epoch {epoch+1}/{epochs} 완료 - 평균 Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # 모델 저장 (매 10 에포크마다)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'text_processor': text_processor\n",
    "            }\n",
    "            save_path = os.path.join(save_dir, f'tacotron2_epoch_{epoch+1}.pth')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'모델 저장: {save_path}')\n",
    "    \n",
    "    print(\"학습 완료!\")\n",
    "    return model, text_processor\n",
    "\n",
    "\n",
    "\n",
    "# 추론 함수\n",
    "def inference_tacotron2(model, text_processor, text, device):\n",
    "    \"\"\"학습된 모델로 음성 합성\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 텍스트 전처리\n",
    "        text_sequence = text_processor.text_to_sequence(text)\n",
    "        text_tensor = torch.LongTensor(text_sequence).unsqueeze(0).to(device)\n",
    "        text_length = torch.LongTensor([len(text_sequence)]).to(device)\n",
    "        \n",
    "        # 인코더 통과\n",
    "        embedded_inputs = model.embedding(text_tensor).transpose(1, 2)\n",
    "        encoder_outputs = model.encoder.inference(text_tensor)\n",
    "        \n",
    "        # 디코더 초기화\n",
    "        memory = encoder_outputs\n",
    "        decoder_input = model.decoder.get_go_frame(memory)\n",
    "        \n",
    "        (model.decoder.attention_hidden, \n",
    "         model.decoder.attention_cell,\n",
    "         model.decoder.decoder_hidden, \n",
    "         model.decoder.decoder_cell,\n",
    "         model.decoder.attention_weights,\n",
    "         model.decoder.attention_weights_cum, \n",
    "         model.decoder.attention_context) = model.decoder.initialize_decoder_states(memory, None)\n",
    "        \n",
    "        model.decoder.memory = memory\n",
    "        model.decoder.processed_memory = model.decoder.attention_layer.memory_layer(memory)\n",
    "        model.decoder.mask = None\n",
    "        \n",
    "        mel_outputs = []\n",
    "        gate_outputs = []\n",
    "        alignments = []\n",
    "        \n",
    "        # 디코딩 루프\n",
    "        while True:\n",
    "            decoder_input = model.decoder.prenet(decoder_input)\n",
    "            mel_output, gate_output, attention_weights = model.decoder.decode(decoder_input)\n",
    "            \n",
    "            mel_outputs.append(mel_output.squeeze(1))\n",
    "            gate_outputs.append(gate_output)\n",
    "            alignments.append(attention_weights)\n",
    "            \n",
    "            # 종료 조건 확인\n",
    "            if torch.sigmoid(gate_output.data) > model.decoder.gate_threshold:\n",
    "                break\n",
    "            elif len(mel_outputs) == model.decoder.max_decoder_steps:\n",
    "                print(\"최대 디코딩 스텝에 도달했습니다.\")\n",
    "                break\n",
    "                \n",
    "            decoder_input = mel_output\n",
    "        \n",
    "        # 출력 정리\n",
    "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        \n",
    "        # 포스트넷 적용\n",
    "        mel_outputs_postnet = model.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs_postnet.squeeze(0).cpu().numpy()\n",
    "\n",
    "# 멜 스펙트로그램을 오디오로 변환 (Griffin-Lim 알고리즘)\n",
    "def mel_to_audio(mel_spectrogram, sr=22050, n_fft=1024, hop_length=256, win_length=1024, n_iter=50):\n",
    "    \"\"\"멜 스펙트로그램을 오디오로 변환\"\"\"\n",
    "    \n",
    "    # 멜 스펙트로그램을 선형 스펙트로그램으로 변환\n",
    "    mel_to_linear_matrix = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=mel_spectrogram.shape[0])\n",
    "    linear_spec = np.dot(mel_to_linear_matrix.T, mel_spectrogram)\n",
    "    \n",
    "    # 로그 스케일에서 원래 스케일로 변환\n",
    "    linear_spec = np.exp(linear_spec) - 1e-9\n",
    "    \n",
    "    # Griffin-Lim 알고리즘으로 위상 복원\n",
    "    audio = librosa.griffinlim(\n",
    "        linear_spec, \n",
    "        n_iter=n_iter, \n",
    "        hop_length=hop_length, \n",
    "        win_length=win_length\n",
    "    )\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# 모델 경량화 함수\n",
    "def quantize_model(model):\n",
    "    \"\"\"모델 양자화로 경량화\"\"\"\n",
    "    model.eval()\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {nn.Linear, nn.Conv1d, nn.LSTM, nn.LSTMCell}, dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model\n",
    "\n",
    "# 모델 프루닝 함수  \n",
    "def prune_model(model, amount=0.2):\n",
    "    \"\"\"모델 프루닝으로 경량화\"\"\"\n",
    "    import torch.nn.utils.prune as prune\n",
    "    \n",
    "    parameters_to_prune = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    # 프루닝된 가중치를 영구적으로 제거\n",
    "    for module, param in parameters_to_prune:\n",
    "        prune.remove(module, param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 디렉토리 스캔 및 데이터 분석 함수\n",
    "def analyze_dataset_directory(json_dir, audio_dir=None):\n",
    "    \"\"\"데이터셋 디렉토리 분석 및 구조 파악\"\"\"\n",
    "    print(f\"📂 디렉토리 분석 시작: {json_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(json_dir):\n",
    "        print(f\"❌ 오류: 디렉토리가 존재하지 않습니다: {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    # JSON 파일 찾기\n",
    "    json_files = []\n",
    "    audio_files = []\n",
    "    \n",
    "    print(\"🔍 파일 스캔 중...\")\n",
    "    file_count = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(json_dir):\n",
    "        for file in files:\n",
    "            file_count += 1\n",
    "            if file_count % 5000 == 0:  # 5000개마다 진행상황 표시\n",
    "                print(f\"📂 스캔 중... {file_count:,}개 파일 확인\")\n",
    "                \n",
    "            if file.lower().endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "            elif file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # 오디오 파일 찾기 (원천데이터 경로에서)\n",
    "    print(\"🎵 오디오 파일 스캔 중...\")\n",
    "    audio_search_dir = json_dir.replace('라벨링데이터', '원천데이터')\n",
    "    if 'TL22' in audio_search_dir:\n",
    "        audio_search_dir = audio_search_dir.replace('TL22', 'TS22')\n",
    "\n",
    "    if os.path.exists(audio_search_dir):\n",
    "        audio_file_count = 0\n",
    "        for root, dirs, files in os.walk(audio_search_dir):\n",
    "            for file in files:\n",
    "                audio_file_count += 1\n",
    "                if audio_file_count % 5000 == 0:\n",
    "                    print(f\"🎵 오디오 스캔 중... {audio_file_count:,}개 파일 확인\")\n",
    "                    \n",
    "                if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "                    audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"📄 JSON 파일 수: {len(json_files):,}\")\n",
    "    print(f\"🎵 오디오 파일 수: {len(audio_files):,}\")\n",
    "    \n",
    "    # 샘플 JSON 파일 구조 분석\n",
    "    if json_files:\n",
    "        print(f\"\\n🔍 샘플 JSON 파일 구조 분석:\")\n",
    "        sample_json = json_files[0]\n",
    "        print(f\"샘플 파일: {sample_json}\")\n",
    "        \n",
    "        try:\n",
    "            with open(sample_json, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            print(\"JSON 구조:\")\n",
    "            print(json.dumps(data, ensure_ascii=False, indent=2)[:500] + \"...\")\n",
    "            \n",
    "            # TransLabelText 찾기\n",
    "            def find_trans_label_text(obj, path=\"\"):\n",
    "                results = []\n",
    "                if isinstance(obj, dict):\n",
    "                    for key, value in obj.items():\n",
    "                        current_path = f\"{path}.{key}\" if path else key\n",
    "                        if key == \"TransLabelText\" and isinstance(value, str):\n",
    "                            results.append((current_path, value))\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            results.extend(find_trans_label_text(value, current_path))\n",
    "                elif isinstance(obj, list):\n",
    "                    for i, item in enumerate(obj):\n",
    "                        current_path = f\"{path}[{i}]\"\n",
    "                        results.extend(find_trans_label_text(item, current_path))\n",
    "                return results\n",
    "            \n",
    "            trans_texts = find_trans_label_text(data)\n",
    "            if trans_texts:\n",
    "                print(f\"\\n✅ TransLabelText 발견:\")\n",
    "                for path, text in trans_texts:\n",
    "                    print(f\"  경로: {path}\")\n",
    "                    print(f\"  텍스트: {text[:50]}...\")\n",
    "            else:\n",
    "                print(f\"\\n❌ TransLabelText를 찾을 수 없습니다.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"JSON 파일 읽기 오류: {e}\")\n",
    "    \n",
    "    # 오디오 파일 분포 확인\n",
    "    if audio_files:\n",
    "        print(f\"\\n🎵 오디오 파일 분포:\")\n",
    "        audio_extensions = {}\n",
    "        for audio_file in audio_files[:100]:  # 처음 100개만 확인\n",
    "            ext = os.path.splitext(audio_file)[1].lower()\n",
    "            audio_extensions[ext] = audio_extensions.get(ext, 0) + 1\n",
    "        \n",
    "        for ext, count in audio_extensions.items():\n",
    "            print(f\"  {ext}: {count}개\")\n",
    "    \n",
    "    # 매칭되는 JSON-오디오 쌍 확인\n",
    "    print(f\"\\n🔗 JSON-오디오 매칭 확인:\")\n",
    "    matched_pairs = 0\n",
    "    for json_file in json_files[:10]:  # 처음 10개만 확인\n",
    "        json_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "        json_dir_path = os.path.dirname(json_file)\n",
    "        \n",
    "        # 개선된 오디오 경로 찾기 로직 사용\n",
    "        audio_path = None\n",
    "        \n",
    "        # 1. 같은 디렉토리에서 찾기\n",
    "        audio_path = os.path.join(json_dir_path, json_name + '.wav')\n",
    "        if os.path.exists(audio_path):\n",
    "            matched_pairs += 1\n",
    "            print(f\"  ✅ 매칭 성공: {os.path.basename(json_file)}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. 라벨링데이터 -> 원천데이터로 경로 변경\n",
    "        if '라벨링데이터' in json_dir_path:\n",
    "            audio_dir_path = json_dir_path.replace('라벨링데이터', '원천데이터')\n",
    "            # TL22 -> TS22 변경\n",
    "            if 'TL22' in audio_dir_path:\n",
    "                audio_dir_path = audio_dir_path.replace('TL22', 'TS22')\n",
    "            \n",
    "            audio_path = os.path.join(audio_dir_path, json_name + '.wav')\n",
    "            if os.path.exists(audio_path):\n",
    "                matched_pairs += 1\n",
    "                print(f\"  ✅ 매칭 성공: {os.path.basename(json_file)} -> 원천데이터\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  ❌ 매칭 실패: {os.path.basename(json_file)}\")\n",
    "\n",
    "    print(f\"  매칭된 쌍: {matched_pairs}/{min(10, len(json_files))} (샘플 10개 중)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"💡 분석 완료!\")\n",
    "    \n",
    "    return {\n",
    "        'json_files': len(json_files),\n",
    "        'audio_files': len(audio_files), \n",
    "        'sample_json': json_files[0] if json_files else None\n",
    "    }\n",
    "\n",
    "\n",
    "# 사용 예시 (업데이트)\n",
    "def main():\n",
    "    # 실제 데이터 경로 설정\n",
    "    json_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.다화자 음성합성 데이터\\01.데이터\\1.Training\\라벨링데이터\\TL22\\TL22\\2.여성\\2400문장\"\n",
    "    audio_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.다화자 음성합성 데이터\\01.데이터\\1.Training\\원천데이터\\TS22\\TS22\\2.여성\\2400문장\"\n",
    "    save_dir = r\"D:\\workspace\\TTS_prj\\models\"\n",
    "    \n",
    "    # 1. 데이터셋 분석\n",
    "    print(\"🔍 데이터셋 분석 중...\")\n",
    "    analyze_dataset_directory(json_dir, audio_dir)\n",
    "    \n",
    "    # 2. 학습 진행 여부 확인\n",
    "    proceed = input(\"\\n학습을 진행하시겠습니까? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        print(\"학습을 취소합니다.\")\n",
    "        return\n",
    "    \n",
    "    # 3. 텍스트 프로세서와 데이터셋 생성 (검증을 위해 미리 생성)\n",
    "    text_processor = KoreanTextProcessor()\n",
    "    dataset = KoreanTTSDataset(json_dir, audio_dir, text_processor)\n",
    "    dataset.data = dataset.data[:100]  # 테스트용으로 축소\n",
    "    \n",
    "    # 4. 인덱스 범위 검증\n",
    "    print(\"=== 인덱스 범위 검증 ===\")\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    print(f\"Vocabulary 크기: {vocab_size}\")\n",
    "    \n",
    "    # 샘플 데이터로 인덱스 확인\n",
    "    sample_texts = [dataset[i]['text'] for i in range(min(10, len(dataset)))]\n",
    "    max_indices = [text.max().item() for text in sample_texts]\n",
    "    print(f\"실제 최대 인덱스들: {max_indices}\")\n",
    "    print(f\"모든 인덱스가 vocabulary 범위 내인가: {all(idx < vocab_size for idx in max_indices)}\")\n",
    "    \n",
    "    if any(idx >= vocab_size for idx in max_indices):\n",
    "        print(\"오류: 인덱스가 vocabulary 범위를 초과합니다!\")\n",
    "        return\n",
    "    \n",
    "    # 5. 학습 시작\n",
    "    print(\"🚀 Tacotron2 모델 학습 시작...\")\n",
    "    model, text_processor = train_tacotron2(\n",
    "        json_dir=json_dir,\n",
    "        audio_dir=audio_dir,\n",
    "        save_dir=save_dir,\n",
    "        epochs=10,\n",
    "        batch_size=1,  # 1로 축소\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if model is None:\n",
    "        print(\"❌ 학습 실패!\")\n",
    "        return\n",
    "    \n",
    "    # 4. 모델 경량화\n",
    "    print(\"⚡ 모델 경량화 중...\")\n",
    "    model = prune_model(model, amount=0.3)\n",
    "    quantized_model = quantize_model(model)\n",
    "    \n",
    "    # 5. 최종 모델 저장\n",
    "    final_save_path = os.path.join(save_dir, 'tacotron2_lightweight_final.pth')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': quantized_model.state_dict(),\n",
    "        'text_processor': text_processor\n",
    "    }, final_save_path)\n",
    "    print(f\"✅ 경량화된 최종 모델 저장: {final_save_path}\")\n",
    "    \n",
    "    # 6. 추론 테스트\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_text = \"안녕하세요. 한국어 음성합성 테스트입니다.\"\n",
    "    \n",
    "    print(f\"🎤 추론 테스트: '{test_text}'\")\n",
    "    mel_output = inference_tacotron2(quantized_model, text_processor, test_text, device)\n",
    "    \n",
    "    # 7. 오디오 생성 및 저장\n",
    "    audio = mel_to_audio(mel_output)\n",
    "    \n",
    "    import soundfile as sf\n",
    "    output_audio_path = os.path.join(save_dir, 'output_test.wav')\n",
    "    sf.write(output_audio_path, audio, 22050)\n",
    "    print(f\"🔊 생성된 오디오 저장: {output_audio_path}\")\n",
    "\n",
    "# 간단한 데이터셋 분석만 실행하는 함수\n",
    "def quick_analysis():\n",
    "    \"\"\"빠른 데이터셋 분석\"\"\"\n",
    "    json_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.다화자 음성합성 데이터\\01.데이터\\1.Training\\라벨링데이터\\TL22\\TL22\\2.여성\\2400문장\"\n",
    "    analyze_dataset_directory(json_dir)\n",
    "\n",
    "# 모델 로드 및 사용 함수\n",
    "def load_and_use_model(model_path, text):\n",
    "    \"\"\"저장된 모델 로드하여 음성 합성\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 체크포인트 로드\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    text_processor = checkpoint['text_processor']\n",
    "    \n",
    "    # 모델 초기화 및 가중치 로드\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    model = Tacotron2(vocab_size).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 추론\n",
    "    mel_output = inference_tacotron2(model, text_processor, text, device)\n",
    "    audio = mel_to_audio(mel_output)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# JSON 파일 구조 확인 함수\n",
    "def check_json_structure(json_file):\n",
    "    \"\"\"JSON 파일의 구조를 확인하여 TransLabelText 위치 파악\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def find_trans_label_text(obj, path=\"\"):\n",
    "        \"\"\"재귀적으로 TransLabelText 찾기\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                current_path = f\"{path}.{key}\" if path else key\n",
    "                if key == \"TransLabelText\":\n",
    "                    print(f\"TransLabelText 발견: {current_path} = {value}\")\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    find_trans_label_text(value, current_path)\n",
    "        elif isinstance(obj, list):\n",
    "            for i, item in enumerate(obj):\n",
    "                current_path = f\"{path}[{i}]\"\n",
    "                find_trans_label_text(item, current_path)\n",
    "    \n",
    "    print(f\"JSON 파일 구조 분석: {json_file}\")\n",
    "    find_trans_label_text(data)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실행 옵션 선택\n",
    "    print(\"🎯 실행 옵션을 선택하세요:\")\n",
    "    print(\"1. 데이터셋 분석만 실행\")\n",
    "    print(\"2. 전체 학습 파이프라인 실행\")\n",
    "    \n",
    "    choice = input(\"선택 (1 또는 2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        quick_analysis()\n",
    "    elif choice == \"2\":\n",
    "        main()\n",
    "    else:\n",
    "        print(\"잘못된 선택입니다. 데이터셋 분석을 실행합니다.\")\n",
    "        quick_analysis()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
