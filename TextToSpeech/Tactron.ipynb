{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a98082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ê²½ëŸ‰í˜• ì˜¨ë””ë°”ì´ìŠ¤ í•œêµ­ì–´ TTS ëª¨ë¸ (Tacotron2 ê¸°ë°˜)\n",
    "JSON íŒŒì¼ì˜ TransLabelTextë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° í•™ìŠµ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "from jamo import h2j, j2hcj\n",
    "import pickle\n",
    "\n",
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
    "class KoreanTextProcessor:\n",
    "    def __init__(self):\n",
    "        # í•œêµ­ì–´ ìëª¨ ë¶„ë¦¬ë¥¼ ìœ„í•œ ë§¤í•‘\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self._build_vocab()\n",
    "    \n",
    "    # KoreanTextProcessor.__init__ ìˆ˜ì •\n",
    "    def _build_vocab(self):\n",
    "        chars = ['<PAD>', '<START>', '<END>', ' ', '!', '?', '.', ',', ';', ':', '-', '(', ')']\n",
    "        cho = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', \n",
    "            'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']\n",
    "        jung = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜',\n",
    "                'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']\n",
    "        jong = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º',\n",
    "                'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……',\n",
    "                'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']\n",
    "        \n",
    "        all_chars = chars + cho + jung + jong\n",
    "        \n",
    "        for i, char in enumerate(all_chars):\n",
    "            self.char_to_id[char] = i\n",
    "            self.id_to_char[i] = char\n",
    "        \n",
    "        print(f\"ì–´íœ˜ í¬ê¸°: {len(all_chars)}\")\n",
    "    \n",
    "    def korean_to_jamo(self, text: str) -> str:\n",
    "        \"\"\"í•œê¸€ì„ ìëª¨ë¡œ ë¶„ë¦¬\"\"\"\n",
    "        result = []\n",
    "        for char in text:\n",
    "            if 'ê°€' <= char <= 'í£':  # í•œê¸€ì¸ ê²½ìš°\n",
    "                decomposed = j2hcj(h2j(char))\n",
    "                result.append(decomposed)\n",
    "            else:\n",
    "                result.append(char)\n",
    "        return ''.join(result)\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™”\"\"\"\n",
    "        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "        text = re.sub(r'[^\\w\\sê°€-í£!?.,;:]', '', text)\n",
    "        # ê³µë°± ì •ë¦¬\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def text_to_sequence(self, text: str) -> List[int]:\n",
    "        text = self.normalize_text(text)\n",
    "        text = self.korean_to_jamo(text)\n",
    "        \n",
    "        sequence = [1]  # START í† í° (<START> = 1)\n",
    "        max_vocab_id = len(self.char_to_id) - 1\n",
    "        \n",
    "        for char in text:\n",
    "            char_id = self.char_to_id.get(char, 0)  # PAD = 0\n",
    "            char_id = min(char_id, max_vocab_id)\n",
    "            sequence.append(char_id)\n",
    "        \n",
    "        sequence.append(2)  # END í† í° (<END> = 2)\n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence: List[int]) -> str:\n",
    "        \"\"\"ì‹œí€€ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        text = []\n",
    "        for id in sequence:\n",
    "            if id in self.id_to_char:\n",
    "                char = self.id_to_char[id]\n",
    "                if char not in ['<PAD>', '<START>', '<END>']:\n",
    "                    text.append(char)\n",
    "        return ''.join(text)\n",
    "\n",
    "# JSON ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "class KoreanTTSDataset(Dataset):\n",
    "    def __init__(self, json_dir: str, audio_dir: str, text_processor: KoreanTextProcessor):\n",
    "        self.text_processor = text_processor\n",
    "        self.audio_dir = audio_dir\n",
    "        self.data = []\n",
    "        \n",
    "        # ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°\n",
    "        json_files = self._find_json_files(json_dir)\n",
    "        print(f\"ì°¾ì€ JSON íŒŒì¼ ìˆ˜: {len(json_files):,}\")\n",
    "        \n",
    "        # JSON íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)\n",
    "        print(\"ğŸ”„ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘...\")\n",
    "        total_files = len(json_files)\n",
    "        \n",
    "        for i, json_file in enumerate(json_files):\n",
    "            self._load_json_data(json_file)\n",
    "            \n",
    "           # ì§„í–‰ë¥  í‘œì‹œ (500ê°œë§ˆë‹¤ ë˜ëŠ” 5%ë§ˆë‹¤) - ì›ë˜ 1000ê°œì—ì„œ 500ê°œë¡œ ë³€ê²½\n",
    "            if (i + 1) % 500 == 0 or (i + 1) % max(1, total_files // 20) == 0:\n",
    "                progress = (i + 1) / total_files * 100\n",
    "                print(f\"ì§„í–‰ë¥ : {i+1:,}/{total_files:,} ({progress:.1f}%) - ìœ íš¨ ë°ì´í„°: {len(self.data):,}ê°œ\")\n",
    "        \n",
    "        print(f\"âœ… JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(self.data):,}ê°œ ë°ì´í„° ë¡œë“œ\")\n",
    "    \n",
    "    def _find_json_files(self, json_dir: str) -> List[str]:\n",
    "        \"\"\"ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ê¸°\"\"\"\n",
    "        json_files = []\n",
    "        \n",
    "        if not os.path.exists(json_dir):\n",
    "            print(f\"ê²½ê³ : ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {json_dir}\")\n",
    "            return json_files\n",
    "        \n",
    "        print(\"ğŸ“‚ JSON íŒŒì¼ ìŠ¤ìº” ì¤‘...\")\n",
    "        \n",
    "        # os.walk()ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°\n",
    "        for root, dirs, files in os.walk(json_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.json'):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    json_files.append(full_path)\n",
    "                    if len(json_files) % 1000 == 0:  # 1000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "                        print(f\"ğŸ“„ JSON íŒŒì¼ ìŠ¤ìº” ì¤‘... {len(json_files):,}ê°œ ë°œê²¬\")\n",
    "        \n",
    "        print(f\"âœ… JSON íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ: {len(json_files):,}ê°œ\")\n",
    "        return sorted(json_files)\n",
    "    \n",
    "    def _load_json_data(self, json_file: str):\n",
    "        \"\"\"JSON íŒŒì¼ì—ì„œ TransLabelText ë°ì´í„° ë¡œë“œ (ë‹¤ì–‘í•œ êµ¬ì¡° ì§€ì›)\"\"\"\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            trans_label_text = None\n",
    "            \n",
    "            # \"ì „ì‚¬ì •ë³´\" í‚¤ì—ì„œ TransLabelText ì°¾ê¸° (ìš°ì„ ìˆœìœ„ 1)\n",
    "            if isinstance(data, dict) and 'ì „ì‚¬ì •ë³´' in data:\n",
    "                if isinstance(data['ì „ì‚¬ì •ë³´'], dict) and 'TransLabelText' in data['ì „ì‚¬ì •ë³´']:\n",
    "                    trans_label_text = data['ì „ì‚¬ì •ë³´']['TransLabelText']\n",
    "                    \n",
    "            # ë§Œì•½ ìœ„ì—ì„œ ëª» ì°¾ì•˜ë‹¤ë©´ ë‹¤ë¥¸ êµ¬ì¡°ë„ í™•ì¸\n",
    "            if not trans_label_text:\n",
    "                if isinstance(data, dict):\n",
    "                    # ì§ì ‘ TransLabelTextê°€ ìˆëŠ” ê²½ìš°\n",
    "                    if 'TransLabelText' in data:\n",
    "                        trans_label_text = data['TransLabelText']\n",
    "                    else:\n",
    "                        # ì¬ê·€ì ìœ¼ë¡œ ì°¾ê¸°\n",
    "                        trans_label_text = self._find_trans_label_text_recursive(data)\n",
    "                elif isinstance(data, list):\n",
    "                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict) and 'ì „ì‚¬ì •ë³´' in item:\n",
    "                            if isinstance(item['ì „ì‚¬ì •ë³´'], dict) and 'TransLabelText' in item['ì „ì‚¬ì •ë³´']:\n",
    "                                trans_label_text = item['ì „ì‚¬ì •ë³´']['TransLabelText']\n",
    "                                break\n",
    "            \n",
    "            # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (JSON íŒŒì¼ëª… ê¸°ë°˜)\n",
    "            if trans_label_text and isinstance(trans_label_text, str) and trans_label_text.strip():\n",
    "                audio_path = self._find_audio_path_improved(json_file)\n",
    "                \n",
    "                self.data.append({\n",
    "                    'text': trans_label_text.strip(),\n",
    "                    'audio': audio_path,\n",
    "                    'json_file': json_file\n",
    "                })\n",
    "                \n",
    "                # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ ìƒíƒœ ì¶œë ¥\n",
    "                if len(self.data) <= 5:\n",
    "                    if audio_path and os.path.exists(audio_path):\n",
    "                        print(f\"âœ… ë§¤ì¹­ ì„±ê³µ: {os.path.basename(json_file)} -> {os.path.basename(audio_path)}\")\n",
    "                    else:\n",
    "                        print(f\"âŒ ì˜¤ë””ì˜¤ ì—†ìŒ: {os.path.basename(json_file)} -> {audio_path}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            if len(self.data) < 3:  # ì²˜ìŒ 3ê°œë§Œ ì˜¤ë¥˜ ì¶œë ¥\n",
    "                print(f\"JSON íŒŒì¼ ë¡œë”© ì˜¤ë¥˜ {os.path.basename(json_file)}: {e}\")\n",
    "    \n",
    "    def _find_trans_label_text_recursive(self, data, max_depth=2, current_depth=0):\n",
    "        \"\"\"ì¬ê·€ì ìœ¼ë¡œ TransLabelText ì°¾ê¸° (ê¹Šì´ ì œí•œ)\"\"\"\n",
    "        if current_depth >= max_depth:\n",
    "            return None\n",
    "            \n",
    "        if isinstance(data, dict):\n",
    "            # ìš°ì„  \"ì „ì‚¬ì •ë³´\" í‚¤ í™•ì¸\n",
    "            if 'ì „ì‚¬ì •ë³´' in data and isinstance(data['ì „ì‚¬ì •ë³´'], dict):\n",
    "                if 'TransLabelText' in data['ì „ì‚¬ì •ë³´']:\n",
    "                    return data['ì „ì‚¬ì •ë³´']['TransLabelText']\n",
    "            \n",
    "            # ì§ì ‘ TransLabelText í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "            if 'TransLabelText' in data:\n",
    "                return data['TransLabelText']\n",
    "            \n",
    "            # ë‹¤ë¥¸ í‚¤ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰ (ê¹Šì´ ì œí•œ)\n",
    "            for key, value in data.items():\n",
    "                if key != 'ê¸°ë³¸ì •ë³´' and isinstance(value, (dict, list)):  # ê¸°ë³¸ì •ë³´ëŠ” ì œì™¸\n",
    "                    result = self._find_trans_label_text_recursive(value, max_depth, current_depth + 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        \n",
    "        elif isinstance(data, list):\n",
    "            for item in data[:3]:  # ë¦¬ìŠ¤íŠ¸ì˜ ì²˜ìŒ 3ê°œë§Œ í™•ì¸\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    result = self._find_trans_label_text_recursive(item, max_depth, current_depth + 1)\n",
    "                    if result:\n",
    "                        return result\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _find_audio_path_improved(self, json_file: str):\n",
    "        \"\"\"ê°œì„ ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°\"\"\"\n",
    "        # JSON íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°\n",
    "        base_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "        json_dir = os.path.dirname(json_file)\n",
    "        \n",
    "        # 1. JSON íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ .wav íŒŒì¼ ì°¾ê¸°\n",
    "        wav_path = os.path.join(json_dir, base_name + '.wav')\n",
    "        if os.path.exists(wav_path):\n",
    "            return wav_path\n",
    "        \n",
    "        # 2. ë¼ë²¨ë§ë°ì´í„° -> ì›ì²œë°ì´í„°ë¡œ ê²½ë¡œ ë³€ê²½\n",
    "        if 'ë¼ë²¨ë§ë°ì´í„°' in json_dir:\n",
    "            audio_dir = json_dir.replace('ë¼ë²¨ë§ë°ì´í„°', 'ì›ì²œë°ì´í„°')\n",
    "            # TL22 -> TS22 ë³€ê²½\n",
    "            if 'TL22' in audio_dir:\n",
    "                audio_dir = audio_dir.replace('TL22', 'TS22')\n",
    "            \n",
    "            wav_path = os.path.join(audio_dir, base_name + '.wav')\n",
    "            if os.path.exists(wav_path):\n",
    "                return wav_path\n",
    "        \n",
    "        # 3. ì§ì ‘ ì§€ì •ëœ audio_dir ì‚¬ìš© (ë§Œì•½ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´)\n",
    "        if hasattr(self, 'audio_dir') and self.audio_dir:\n",
    "            # JSONì˜ ìƒëŒ€ ê²½ë¡œ êµ¬ì¡°ë¥¼ audio_dirì— ì ìš©\n",
    "            json_relative_parts = json_dir.split(os.sep)\n",
    "            # 0001_G2A2E7_KMJ ê°™ì€ í´ë”ëª… ì°¾ê¸°\n",
    "            speaker_folder = None\n",
    "            for part in json_relative_parts:\n",
    "                if '_G2A2E7_' in part:  # í™”ì í´ë” íŒ¨í„´\n",
    "                    speaker_folder = part\n",
    "                    break\n",
    "            \n",
    "            if speaker_folder:\n",
    "                wav_path = os.path.join(self.audio_dir, speaker_folder, base_name + '.wav')\n",
    "                if os.path.exists(wav_path):\n",
    "                    return wav_path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        text = item['text']\n",
    "        text_sequence = self.text_processor.text_to_sequence(text)\n",
    "        \n",
    "        # ì˜¤ë””ì˜¤ ê²½ë¡œê°€ ì´ë¯¸ ì „ì²´ ê²½ë¡œì„\n",
    "        mel_spectrogram = self._process_audio(item['audio'])  # self.audio_dir ì œê±°\n",
    "        \n",
    "        return {\n",
    "            'text': torch.LongTensor(text_sequence),\n",
    "            'mel': torch.FloatTensor(mel_spectrogram),\n",
    "            'text_length': len(text_sequence),\n",
    "            'mel_length': mel_spectrogram.shape[1]\n",
    "        }\n",
    "    \n",
    "    def _process_audio(self, audio_path: str) -> np.ndarray:\n",
    "        \"\"\"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        try:\n",
    "            # ì˜¤ë””ì˜¤ ë¡œë“œ\n",
    "            audio, sr = librosa.load(audio_path, sr=22050)\n",
    "            \n",
    "            # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±\n",
    "            mel = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=sr,\n",
    "                n_fft=1024,\n",
    "                hop_length=256,\n",
    "                win_length=1024,\n",
    "                n_mels=80,\n",
    "                fmin=0,\n",
    "                fmax=8000\n",
    "            )\n",
    "            \n",
    "            # ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³€í™˜\n",
    "            mel = np.log(mel + 1e-9)\n",
    "            \n",
    "            return mel\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ {audio_path}: {e}\")\n",
    "            # ë”ë¯¸ ë°ì´í„° ë°˜í™˜\n",
    "            return np.zeros((80, 100))\n",
    "\n",
    "# Tacotron2 ëª¨ë¸ êµ¬ì„±ìš”ì†Œë“¤\n",
    "class LocationLayer(nn.Module):\n",
    "    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):\n",
    "        super(LocationLayer, self).__init__()\n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.location_conv = nn.Conv1d(2, attention_n_filters, kernel_size=attention_kernel_size, padding=padding, bias=False)\n",
    "        self.location_dense = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "    \n",
    "    def forward(self, attention_weights_cat):\n",
    "        processed_attention = self.location_conv(attention_weights_cat)\n",
    "        processed_attention = processed_attention.transpose(1, 2)\n",
    "        processed_attention = self.location_dense(processed_attention)\n",
    "        return processed_attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = nn.Linear(attention_rnn_dim, attention_dim, bias=False)\n",
    "        self.memory_layer = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters, attention_location_kernel_size, attention_dim)\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "    \n",
    "    def get_alignment_energies(self, query, processed_memory, attention_weights_cat):\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        energies = self.v(torch.tanh(processed_query + processed_attention_weights + processed_memory))\n",
    "        energies = energies.squeeze(-1)\n",
    "        return energies\n",
    "    \n",
    "    def forward(self, attention_hidden_state, memory, processed_memory, attention_weights_cat, mask):\n",
    "        alignment = self.get_alignment_energies(attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "        \n",
    "        if mask is not None:\n",
    "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "        \n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "        \n",
    "        return attention_context, attention_weights\n",
    "\n",
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes):\n",
    "        super(Prenet, self).__init__()\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_size, out_size) for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "        return x\n",
    "\n",
    "class Postnet(nn.Module):\n",
    "    def __init__(self, mel_dim, postnet_embedding_dim, postnet_kernel_size, postnet_n_convolutions):\n",
    "        super(Postnet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        \n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(mel_dim, postnet_embedding_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for i in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_embedding_dim, postnet_embedding_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(postnet_embedding_dim, mel_dim, kernel_size=postnet_kernel_size, stride=1, padding=int((postnet_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(mel_dim)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
    "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_n_convolutions, encoder_embedding_dim, encoder_kernel_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        std = np.sqrt(2.0 / (vocab_size + embedding_dim))\n",
    "        val = np.sqrt(3.0) * std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        \n",
    "        convolutions = []\n",
    "        for _ in range(encoder_n_convolutions):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(embedding_dim, encoder_embedding_dim, kernel_size=encoder_kernel_size, stride=1, padding=int((encoder_kernel_size - 1) / 2), dilation=1, bias=False),\n",
    "                nn.BatchNorm1d(encoder_embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "            convolutions.append(conv_layer)\n",
    "            embedding_dim = encoder_embedding_dim\n",
    "        \n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        self.lstm = nn.LSTM(encoder_embedding_dim, int(encoder_embedding_dim // 2), 1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        input_lengths = input_lengths.cpu().numpy()\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def inference(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, mel_dim, encoder_embedding_dim, attention_rnn_dim, decoder_rnn_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size, prenet_dim, max_decoder_steps, gate_threshold, p_attention_dropout, p_decoder_dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mel_dim = mel_dim\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.attention_rnn_dim = attention_rnn_dim\n",
    "        self.decoder_rnn_dim = decoder_rnn_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.attention_location_n_filters = attention_location_n_filters\n",
    "        self.attention_location_kernel_size = attention_location_kernel_size\n",
    "        self.prenet_dim = prenet_dim\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.gate_threshold = gate_threshold\n",
    "        self.p_attention_dropout = p_attention_dropout\n",
    "        self.p_decoder_dropout = p_decoder_dropout\n",
    "        \n",
    "        self.prenet = Prenet(mel_dim, [prenet_dim, prenet_dim])\n",
    "        \n",
    "        self.attention_rnn = nn.LSTMCell(prenet_dim + encoder_embedding_dim, attention_rnn_dim)\n",
    "        \n",
    "        self.attention_layer = Attention(attention_rnn_dim, encoder_embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size)\n",
    "        \n",
    "        self.decoder_rnn = nn.LSTMCell(attention_rnn_dim + encoder_embedding_dim, decoder_rnn_dim, 1)\n",
    "        \n",
    "        self.linear_projection = nn.Linear(decoder_rnn_dim + encoder_embedding_dim, mel_dim)\n",
    "        \n",
    "        self.gate_layer = nn.Linear(decoder_rnn_dim + encoder_embedding_dim, 1, bias=True)\n",
    "    \n",
    "    def get_go_frame(self, memory):\n",
    "        B = memory.size(0)\n",
    "        go_frame = torch.zeros(B, self.mel_dim, device=memory.device, dtype=memory.dtype)\n",
    "        return go_frame\n",
    "    \n",
    "    # def initialize_decoder_states(self, memory, mask):\n",
    "    #     B = memory.size(0)\n",
    "    #     MAX_TIME = memory.size(1)\n",
    "        \n",
    "    #     attention_hidden = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_cell = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     decoder_hidden = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "    #     decoder_cell = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     attention_weights = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_weights_cum = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "    #     attention_context = torch.zeros(B, self.encoder_embedding_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "    #     return (attention_hidden, attention_cell, decoder_hidden, decoder_cell, attention_weights, attention_weights_cum, attention_context)\n",
    "    def initialize_decoder_states(self, memory):\n",
    "        \"\"\"ë””ì½”ë” ìƒíƒœ ì´ˆê¸°í™”\"\"\"\n",
    "        B = memory.size(0)\n",
    "        MAX_TIME = memory.size(1)\n",
    "        \n",
    "        self.attention_hidden = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_cell = torch.zeros(B, self.attention_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.decoder_hidden = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        self.decoder_cell = torch.zeros(B, self.decoder_rnn_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.attention_weights = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_weights_cum = torch.zeros(B, MAX_TIME, device=memory.device, dtype=memory.dtype)\n",
    "        self.attention_context = torch.zeros(B, self.encoder_embedding_dim, device=memory.device, dtype=memory.dtype)\n",
    "        \n",
    "        self.memory = memory\n",
    "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "        self.mask = None\n",
    "    \n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        decoder_inputs = decoder_inputs.view(decoder_inputs.size(0), int(decoder_inputs.size(1) / self.mel_dim), self.mel_dim)\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        return decoder_inputs\n",
    "    \n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "        gate_outputs = gate_outputs.contiguous()\n",
    "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "        mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.mel_dim)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "    \n",
    "    def decode(self, decoder_input):\n",
    "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "        \n",
    "        self.attention_hidden, self.attention_cell = self.attention_rnn(cell_input, (self.attention_hidden, self.attention_cell))\n",
    "        self.attention_hidden = F.dropout(self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "        \n",
    "        attention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "        self.attention_context, self.attention_weights = self.attention_layer(self.attention_hidden, self.memory, self.processed_memory, attention_weights_cat, self.mask)\n",
    "        \n",
    "        self.attention_weights_cum += self.attention_weights\n",
    "        decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)\n",
    "        \n",
    "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "        self.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "        \n",
    "        decoder_hidden_attention_context = torch.cat((self.decoder_hidden, self.attention_context), dim=1)\n",
    "        decoder_output = self.linear_projection(decoder_hidden_attention_context)\n",
    "        \n",
    "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "        \n",
    "        return decoder_output, gate_prediction, self.attention_weights\n",
    "    \n",
    "    # ê¸°ì¡´ ì½”ë“œ\n",
    "    # def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "    #     decoder_input = self.get_go_frame(memory).unsqueeze(1)\n",
    "    #     decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "    #     decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=1)\n",
    "    #     decoder_inputs = self.prenet(decoder_inputs)\n",
    "        \n",
    "    #     self.initialize_decoder_states(memory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "        \n",
    "    #     mel_outputs, gate_outputs, alignments = [], [], []\n",
    "    #     while len(mel_outputs) < decoder_inputs.size(1) - 1:\n",
    "    #         decoder_input = decoder_inputs[:, len(mel_outputs)]\n",
    "    #         mel_output, gate_output, attention_weights = self.decode(decoder_input)\n",
    "    #         mel_outputs += [mel_output.squeeze(1)]\n",
    "    #         gate_outputs += [gate_output.squeeze(1)]\n",
    "    #         alignments += [attention_weights]\n",
    "        \n",
    "    #     mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(mel_outputs, gate_outputs, alignments)\n",
    "        \n",
    "    #     return mel_outputs, gate_outputs, alignments\n",
    "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "        # ê°„ë‹¨í•œ ë”ë¯¸ ì¶œë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "        batch_size = memory.size(0)\n",
    "        max_mel_len = decoder_inputs.size(1) // self.mel_dim\n",
    "        \n",
    "        # ë”ë¯¸ ì¶œë ¥ ìƒì„±\n",
    "        mel_outputs = torch.randn(batch_size, self.mel_dim, max_mel_len, device=memory.device)\n",
    "        gate_outputs = torch.zeros(batch_size, max_mel_len, device=memory.device)\n",
    "        alignments = torch.zeros(batch_size, max_mel_len, memory.size(1), device=memory.device)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "# Tacotron2 ë©”ì¸ ëª¨ë¸\n",
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        \n",
    "        # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê²½ëŸ‰í™”ë¥¼ ìœ„í•´ í¬ê¸° ì¶•ì†Œ)\n",
    "        self.embedding_dim = 256  # ì›ë˜ 512ì—ì„œ ì¶•ì†Œ\n",
    "        self.encoder_embedding_dim = 256  # ì›ë˜ 512ì—ì„œ ì¶•ì†Œ\n",
    "        self.encoder_n_convolutions = 3\n",
    "        self.encoder_kernel_size = 5\n",
    "        self.attention_rnn_dim = 512  # ì›ë˜ 1024ì—ì„œ ì¶•ì†Œ\n",
    "        self.attention_dim = 64  # ì›ë˜ 128ì—ì„œ ì¶•ì†Œ\n",
    "        self.attention_location_n_filters = 16  # ì›ë˜ 32ì—ì„œ ì¶•ì†Œ\n",
    "        self.attention_location_kernel_size = 31\n",
    "        self.decoder_rnn_dim = 512  # ì›ë˜ 1024ì—ì„œ ì¶•ì†Œ\n",
    "        self.prenet_dim = 128  # ì›ë˜ 256ì—ì„œ ì¶•ì†Œ\n",
    "        self.max_decoder_steps = 1000\n",
    "        self.gate_threshold = 0.5\n",
    "        self.p_attention_dropout = 0.1\n",
    "        self.p_decoder_dropout = 0.1\n",
    "        self.postnet_embedding_dim = 256  # ì›ë˜ 512ì—ì„œ ì¶•ì†Œ\n",
    "        self.postnet_kernel_size = 5\n",
    "        self.postnet_n_convolutions = 5\n",
    "        self.mel_dim = 80\n",
    "        safe_vocab_size = max(vocab_size, 100)\n",
    "        \n",
    "        # self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.embedding = nn.Embedding(safe_vocab_size, self.embedding_dim)\n",
    "        std = np.sqrt(2.0 / (safe_vocab_size + self.embedding_dim))\n",
    "        val = np.sqrt(3.0) * std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        \n",
    "        self.encoder = Encoder(safe_vocab_size, self.embedding_dim, self.encoder_n_convolutions, self.encoder_embedding_dim, self.encoder_kernel_size)\n",
    "        \n",
    "        self.decoder = Decoder(self.mel_dim, self.encoder_embedding_dim, self.attention_rnn_dim, self.decoder_rnn_dim, self.attention_dim, self.attention_location_n_filters, self.attention_location_kernel_size, self.prenet_dim, self.max_decoder_steps, self.gate_threshold, self.p_attention_dropout, self.p_decoder_dropout)\n",
    "        \n",
    "        self.postnet = Postnet(self.mel_dim, self.postnet_embedding_dim, self.postnet_kernel_size, self.postnet_n_convolutions)\n",
    "    \n",
    "    def forward(self, text_inputs, text_lengths, mels, mel_lengths):\n",
    "        text_lengths, mel_lengths = text_lengths.data, mel_lengths.data\n",
    "        \n",
    "        # ì´ ë¼ì¸ì„ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.\n",
    "        # embedded_inputs = self.embedding(text_inputs).transpose(1, 2) \n",
    "        \n",
    "        # text_inputsë¥¼ encoderì— ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "        encoder_outputs = self.encoder(text_inputs, text_lengths)\n",
    "        \n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(encoder_outputs, mels, text_lengths)\n",
    "        \n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n",
    "\n",
    "\n",
    "# def get_mask_from_lengths(lengths):\n",
    "#     max_len = torch.max(lengths).item()\n",
    "#     ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
    "#     mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "#     return mask\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device)  # device ëª…ì‹œ\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask\n",
    "\n",
    "# collate_fn ì‹œê·¸ë‹ˆì²˜ ìˆ˜ì • ë° ë¡œì§ ë³€ê²½\n",
    "def collate_fn(batch, vocab_size): # vocab_sizeë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì •\n",
    "    batch.sort(key=lambda x: x['text_length'], reverse=True)\n",
    "    \n",
    "    texts = [item['text'] for item in batch]\n",
    "    mels = [item['mel'] for item in batch]\n",
    "    text_lengths = torch.LongTensor([len(item['text']) for item in batch])\n",
    "    mel_lengths = torch.LongTensor([item['mel'].shape[1] for item in batch])\n",
    "    \n",
    "    # ì¸ë±ìŠ¤ ê²€ì¦ (ë™ì ìœ¼ë¡œ vocab_size ì‚¬ìš©)\n",
    "    max_valid_index = vocab_size - 1\n",
    "    for i, text in enumerate(texts):\n",
    "        if text.numel() > 0 and text.max() > max_valid_index:\n",
    "            print(f\"ê²½ê³ : í…ìŠ¤íŠ¸ {i}ì—ì„œ ë²”ìœ„ ì´ˆê³¼ ì¸ë±ìŠ¤({text.max()})ê°€ ì–´íœ˜ì‚¬ì „ í¬ê¸°({vocab_size})ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. PAD(0)ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\")\n",
    "            text[text > max_valid_index] = 0  # ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ì„ PAD í† í°(0)ìœ¼ë¡œ ê°•ì œ ë³€í™˜\n",
    "            texts[i] = text\n",
    "\n",
    "    # íŒŒì´í† ì¹˜ ë‚´ì¥ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ íŒ¨ë”©\n",
    "    text_padded = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŒ¨ë”©\n",
    "    mel_padded = torch.zeros(len(batch), 80, mel_lengths.max())\n",
    "    for i, mel in enumerate(mels):\n",
    "        mel_padded[i, :, :mel.shape[1]] = mel\n",
    "    \n",
    "    return text_padded, mel_padded, text_lengths, mel_lengths\n",
    "\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_tacotron2(json_dir, audio_dir, save_dir, epochs=100, batch_size=8, lr=1e-3):\n",
    "    \"\"\"Tacotron2 ëª¨ë¸ í•™ìŠµ (ë””ë ‰í† ë¦¬ ê¸°ë°˜)\"\"\"\n",
    "    from functools import partial\n",
    "    \n",
    "    # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”\n",
    "    text_processor = KoreanTextProcessor()\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    print(f\"JSON íŒŒì¼ ë””ë ‰í† ë¦¬: {json_dir}\")\n",
    "    print(f\"ì˜¤ë””ì˜¤ íŒŒì¼ ë””ë ‰í† ë¦¬: {audio_dir}\")\n",
    "    \n",
    "    dataset = KoreanTTSDataset(json_dir, audio_dir, text_processor)\n",
    "    dataset.data = dataset.data[:100]  # ì²˜ìŒ 10,000ê°œë§Œ ì‚¬ìš©\n",
    "\n",
    "    collate_with_vocab = partial(collate_fn, vocab_size=vocab_size)\n",
    "\n",
    "    \n",
    "    # dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                        collate_fn=collate_with_vocab, num_workers=0)\n",
    "    \n",
    "    print(f\"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}\")\n",
    "    print(f\"vocabulary í¬ê¸°: {vocab_size}\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. JSON íŒŒì¼ê³¼ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return None, None\n",
    "    \n",
    "    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "    print(\"\\n=== ë°ì´í„° ìƒ˜í”Œ í™•ì¸ ===\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        sample = dataset.data[i]\n",
    "        print(f\"ìƒ˜í”Œ {i+1}:\")\n",
    "        print(f\"  í…ìŠ¤íŠ¸: {sample['text'][:50]}...\")\n",
    "        print(f\"  ì˜¤ë””ì˜¤: {sample['audio']}\")\n",
    "        print(f\"  JSON: {sample['json_file']}\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = Tacotron2(vocab_size).to(device)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì €\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_batches = len(dataloader)\n",
    "        for batch_idx, (text_padded, mel_padded, text_lengths, mel_lengths) in enumerate(dataloader):\n",
    "            text_padded = text_padded.to(device)\n",
    "            mel_padded = mel_padded.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            mel_lengths = mel_lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model(\n",
    "                text_padded, text_lengths, mel_padded, mel_lengths\n",
    "            )\n",
    "            \n",
    "            # ì†ì‹¤ ê³„ì‚°\n",
    "            mel_loss = criterion(mel_outputs, mel_padded)\n",
    "            mel_postnet_loss = criterion(mel_outputs_postnet, mel_padded)\n",
    "            gate_loss = nn.BCEWithLogitsLoss()(gate_outputs, torch.zeros_like(gate_outputs))\n",
    "            \n",
    "            total_batch_loss = mel_loss + mel_postnet_loss + gate_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_batch_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += total_batch_loss.item()\n",
    "            \n",
    "            if batch_idx % max(1, total_batches // 10) == 0 or (batch_idx + 1) % 5 == 0:\n",
    "                progress = (batch_idx + 1) / total_batches * 100\n",
    "                print(f\"ğŸ“Š Batch {batch_idx+1:3d}/{total_batches} ({progress:5.1f}%) - Loss: {total_batch_loss.item():.4f}\")\n",
    "    \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"âœ… Epoch {epoch+1}/{epochs} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥ (ë§¤ 10 ì—í¬í¬ë§ˆë‹¤)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'text_processor': text_processor\n",
    "            }\n",
    "            save_path = os.path.join(save_dir, f'tacotron2_epoch_{epoch+1}.pth')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'ëª¨ë¸ ì €ì¥: {save_path}')\n",
    "    \n",
    "    print(\"í•™ìŠµ ì™„ë£Œ!\")\n",
    "    return model, text_processor\n",
    "\n",
    "\n",
    "\n",
    "# ì¶”ë¡  í•¨ìˆ˜\n",
    "def inference_tacotron2(model, text_processor, text, device):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ë¡œ ìŒì„± í•©ì„±\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "        text_sequence = text_processor.text_to_sequence(text)\n",
    "        text_tensor = torch.LongTensor(text_sequence).unsqueeze(0).to(device)\n",
    "        text_length = torch.LongTensor([len(text_sequence)]).to(device)\n",
    "        \n",
    "        # ì¸ì½”ë” í†µê³¼\n",
    "        embedded_inputs = model.embedding(text_tensor).transpose(1, 2)\n",
    "        encoder_outputs = model.encoder.inference(text_tensor)\n",
    "        \n",
    "        # ë””ì½”ë” ì´ˆê¸°í™”\n",
    "        memory = encoder_outputs\n",
    "        decoder_input = model.decoder.get_go_frame(memory)\n",
    "        \n",
    "        (model.decoder.attention_hidden, \n",
    "         model.decoder.attention_cell,\n",
    "         model.decoder.decoder_hidden, \n",
    "         model.decoder.decoder_cell,\n",
    "         model.decoder.attention_weights,\n",
    "         model.decoder.attention_weights_cum, \n",
    "         model.decoder.attention_context) = model.decoder.initialize_decoder_states(memory, None)\n",
    "        \n",
    "        model.decoder.memory = memory\n",
    "        model.decoder.processed_memory = model.decoder.attention_layer.memory_layer(memory)\n",
    "        model.decoder.mask = None\n",
    "        \n",
    "        mel_outputs = []\n",
    "        gate_outputs = []\n",
    "        alignments = []\n",
    "        \n",
    "        # ë””ì½”ë”© ë£¨í”„\n",
    "        while True:\n",
    "            decoder_input = model.decoder.prenet(decoder_input)\n",
    "            mel_output, gate_output, attention_weights = model.decoder.decode(decoder_input)\n",
    "            \n",
    "            mel_outputs.append(mel_output.squeeze(1))\n",
    "            gate_outputs.append(gate_output)\n",
    "            alignments.append(attention_weights)\n",
    "            \n",
    "            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸\n",
    "            if torch.sigmoid(gate_output.data) > model.decoder.gate_threshold:\n",
    "                break\n",
    "            elif len(mel_outputs) == model.decoder.max_decoder_steps:\n",
    "                print(\"ìµœëŒ€ ë””ì½”ë”© ìŠ¤í…ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                break\n",
    "                \n",
    "            decoder_input = mel_output\n",
    "        \n",
    "        # ì¶œë ¥ ì •ë¦¬\n",
    "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        \n",
    "        # í¬ìŠ¤íŠ¸ë„· ì ìš©\n",
    "        mel_outputs_postnet = model.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs_postnet.squeeze(0).cpu().numpy()\n",
    "\n",
    "# ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ (Griffin-Lim ì•Œê³ ë¦¬ì¦˜)\n",
    "def mel_to_audio(mel_spectrogram, sr=22050, n_fft=1024, hop_length=256, win_length=1024, n_iter=50):\n",
    "    \"\"\"ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜\"\"\"\n",
    "    \n",
    "    # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì„ í˜• ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜\n",
    "    mel_to_linear_matrix = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=mel_spectrogram.shape[0])\n",
    "    linear_spec = np.dot(mel_to_linear_matrix.T, mel_spectrogram)\n",
    "    \n",
    "    # ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜\n",
    "    linear_spec = np.exp(linear_spec) - 1e-9\n",
    "    \n",
    "    # Griffin-Lim ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìœ„ìƒ ë³µì›\n",
    "    audio = librosa.griffinlim(\n",
    "        linear_spec, \n",
    "        n_iter=n_iter, \n",
    "        hop_length=hop_length, \n",
    "        win_length=win_length\n",
    "    )\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# ëª¨ë¸ ê²½ëŸ‰í™” í•¨ìˆ˜\n",
    "def quantize_model(model):\n",
    "    \"\"\"ëª¨ë¸ ì–‘ìí™”ë¡œ ê²½ëŸ‰í™”\"\"\"\n",
    "    model.eval()\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model, {nn.Linear, nn.Conv1d, nn.LSTM, nn.LSTMCell}, dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model\n",
    "\n",
    "# ëª¨ë¸ í”„ë£¨ë‹ í•¨ìˆ˜  \n",
    "def prune_model(model, amount=0.2):\n",
    "    \"\"\"ëª¨ë¸ í”„ë£¨ë‹ìœ¼ë¡œ ê²½ëŸ‰í™”\"\"\"\n",
    "    import torch.nn.utils.prune as prune\n",
    "    \n",
    "    parameters_to_prune = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    # í”„ë£¨ë‹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì œê±°\n",
    "    for module, param in parameters_to_prune:\n",
    "        prune.remove(module, param)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ë°ì´í„° ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_dataset_directory(json_dir, audio_dir=None):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ë¶„ì„ ë° êµ¬ì¡° íŒŒì•…\"\"\"\n",
    "    print(f\"ğŸ“‚ ë””ë ‰í† ë¦¬ ë¶„ì„ ì‹œì‘: {json_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(json_dir):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    # JSON íŒŒì¼ ì°¾ê¸°\n",
    "    json_files = []\n",
    "    audio_files = []\n",
    "    \n",
    "    print(\"ğŸ” íŒŒì¼ ìŠ¤ìº” ì¤‘...\")\n",
    "    file_count = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(json_dir):\n",
    "        for file in files:\n",
    "            file_count += 1\n",
    "            if file_count % 5000 == 0:  # 5000ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "                print(f\"ğŸ“‚ ìŠ¤ìº” ì¤‘... {file_count:,}ê°œ íŒŒì¼ í™•ì¸\")\n",
    "                \n",
    "            if file.lower().endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "            elif file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (ì›ì²œë°ì´í„° ê²½ë¡œì—ì„œ)\n",
    "    print(\"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤ìº” ì¤‘...\")\n",
    "    audio_search_dir = json_dir.replace('ë¼ë²¨ë§ë°ì´í„°', 'ì›ì²œë°ì´í„°')\n",
    "    if 'TL22' in audio_search_dir:\n",
    "        audio_search_dir = audio_search_dir.replace('TL22', 'TS22')\n",
    "\n",
    "    if os.path.exists(audio_search_dir):\n",
    "        audio_file_count = 0\n",
    "        for root, dirs, files in os.walk(audio_search_dir):\n",
    "            for file in files:\n",
    "                audio_file_count += 1\n",
    "                if audio_file_count % 5000 == 0:\n",
    "                    print(f\"ğŸµ ì˜¤ë””ì˜¤ ìŠ¤ìº” ì¤‘... {audio_file_count:,}ê°œ íŒŒì¼ í™•ì¸\")\n",
    "                    \n",
    "                if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "                    audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"ğŸ“„ JSON íŒŒì¼ ìˆ˜: {len(json_files):,}\")\n",
    "    print(f\"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜: {len(audio_files):,}\")\n",
    "    \n",
    "    # ìƒ˜í”Œ JSON íŒŒì¼ êµ¬ì¡° ë¶„ì„\n",
    "    if json_files:\n",
    "        print(f\"\\nğŸ” ìƒ˜í”Œ JSON íŒŒì¼ êµ¬ì¡° ë¶„ì„:\")\n",
    "        sample_json = json_files[0]\n",
    "        print(f\"ìƒ˜í”Œ íŒŒì¼: {sample_json}\")\n",
    "        \n",
    "        try:\n",
    "            with open(sample_json, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            print(\"JSON êµ¬ì¡°:\")\n",
    "            print(json.dumps(data, ensure_ascii=False, indent=2)[:500] + \"...\")\n",
    "            \n",
    "            # TransLabelText ì°¾ê¸°\n",
    "            def find_trans_label_text(obj, path=\"\"):\n",
    "                results = []\n",
    "                if isinstance(obj, dict):\n",
    "                    for key, value in obj.items():\n",
    "                        current_path = f\"{path}.{key}\" if path else key\n",
    "                        if key == \"TransLabelText\" and isinstance(value, str):\n",
    "                            results.append((current_path, value))\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            results.extend(find_trans_label_text(value, current_path))\n",
    "                elif isinstance(obj, list):\n",
    "                    for i, item in enumerate(obj):\n",
    "                        current_path = f\"{path}[{i}]\"\n",
    "                        results.extend(find_trans_label_text(item, current_path))\n",
    "                return results\n",
    "            \n",
    "            trans_texts = find_trans_label_text(data)\n",
    "            if trans_texts:\n",
    "                print(f\"\\nâœ… TransLabelText ë°œê²¬:\")\n",
    "                for path, text in trans_texts:\n",
    "                    print(f\"  ê²½ë¡œ: {path}\")\n",
    "                    print(f\"  í…ìŠ¤íŠ¸: {text[:50]}...\")\n",
    "            else:\n",
    "                print(f\"\\nâŒ TransLabelTextë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„í¬ í™•ì¸\n",
    "    if audio_files:\n",
    "        print(f\"\\nğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„í¬:\")\n",
    "        audio_extensions = {}\n",
    "        for audio_file in audio_files[:100]:  # ì²˜ìŒ 100ê°œë§Œ í™•ì¸\n",
    "            ext = os.path.splitext(audio_file)[1].lower()\n",
    "            audio_extensions[ext] = audio_extensions.get(ext, 0) + 1\n",
    "        \n",
    "        for ext, count in audio_extensions.items():\n",
    "            print(f\"  {ext}: {count}ê°œ\")\n",
    "    \n",
    "    # ë§¤ì¹­ë˜ëŠ” JSON-ì˜¤ë””ì˜¤ ìŒ í™•ì¸\n",
    "    print(f\"\\nğŸ”— JSON-ì˜¤ë””ì˜¤ ë§¤ì¹­ í™•ì¸:\")\n",
    "    matched_pairs = 0\n",
    "    for json_file in json_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸\n",
    "        json_name = os.path.splitext(os.path.basename(json_file))[0]\n",
    "        json_dir_path = os.path.dirname(json_file)\n",
    "        \n",
    "        # ê°œì„ ëœ ì˜¤ë””ì˜¤ ê²½ë¡œ ì°¾ê¸° ë¡œì§ ì‚¬ìš©\n",
    "        audio_path = None\n",
    "        \n",
    "        # 1. ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°\n",
    "        audio_path = os.path.join(json_dir_path, json_name + '.wav')\n",
    "        if os.path.exists(audio_path):\n",
    "            matched_pairs += 1\n",
    "            print(f\"  âœ… ë§¤ì¹­ ì„±ê³µ: {os.path.basename(json_file)}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. ë¼ë²¨ë§ë°ì´í„° -> ì›ì²œë°ì´í„°ë¡œ ê²½ë¡œ ë³€ê²½\n",
    "        if 'ë¼ë²¨ë§ë°ì´í„°' in json_dir_path:\n",
    "            audio_dir_path = json_dir_path.replace('ë¼ë²¨ë§ë°ì´í„°', 'ì›ì²œë°ì´í„°')\n",
    "            # TL22 -> TS22 ë³€ê²½\n",
    "            if 'TL22' in audio_dir_path:\n",
    "                audio_dir_path = audio_dir_path.replace('TL22', 'TS22')\n",
    "            \n",
    "            audio_path = os.path.join(audio_dir_path, json_name + '.wav')\n",
    "            if os.path.exists(audio_path):\n",
    "                matched_pairs += 1\n",
    "                print(f\"  âœ… ë§¤ì¹­ ì„±ê³µ: {os.path.basename(json_file)} -> ì›ì²œë°ì´í„°\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  âŒ ë§¤ì¹­ ì‹¤íŒ¨: {os.path.basename(json_file)}\")\n",
    "\n",
    "    print(f\"  ë§¤ì¹­ëœ ìŒ: {matched_pairs}/{min(10, len(json_files))} (ìƒ˜í”Œ 10ê°œ ì¤‘)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ’¡ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    \n",
    "    return {\n",
    "        'json_files': len(json_files),\n",
    "        'audio_files': len(audio_files), \n",
    "        'sample_json': json_files[0] if json_files else None\n",
    "    }\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ì—…ë°ì´íŠ¸)\n",
    "def main():\n",
    "    # ì‹¤ì œ ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    json_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.ë‹¤í™”ì ìŒì„±í•©ì„± ë°ì´í„°\\01.ë°ì´í„°\\1.Training\\ë¼ë²¨ë§ë°ì´í„°\\TL22\\TL22\\2.ì—¬ì„±\\2400ë¬¸ì¥\"\n",
    "    audio_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.ë‹¤í™”ì ìŒì„±í•©ì„± ë°ì´í„°\\01.ë°ì´í„°\\1.Training\\ì›ì²œë°ì´í„°\\TS22\\TS22\\2.ì—¬ì„±\\2400ë¬¸ì¥\"\n",
    "    save_dir = r\"D:\\workspace\\TTS_prj\\models\"\n",
    "    \n",
    "    # 1. ë°ì´í„°ì…‹ ë¶„ì„\n",
    "    print(\"ğŸ” ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘...\")\n",
    "    analyze_dataset_directory(json_dir, audio_dir)\n",
    "    \n",
    "    # 2. í•™ìŠµ ì§„í–‰ ì—¬ë¶€ í™•ì¸\n",
    "    proceed = input(\"\\ní•™ìŠµì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        print(\"í•™ìŠµì„ ì·¨ì†Œí•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # 3. í…ìŠ¤íŠ¸ í”„ë¡œì„¸ì„œì™€ ë°ì´í„°ì…‹ ìƒì„± (ê²€ì¦ì„ ìœ„í•´ ë¯¸ë¦¬ ìƒì„±)\n",
    "    text_processor = KoreanTextProcessor()\n",
    "    dataset = KoreanTTSDataset(json_dir, audio_dir, text_processor)\n",
    "    dataset.data = dataset.data[:100]  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¶•ì†Œ\n",
    "    \n",
    "    # 4. ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦\n",
    "    print(\"=== ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦ ===\")\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    print(f\"Vocabulary í¬ê¸°: {vocab_size}\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ë°ì´í„°ë¡œ ì¸ë±ìŠ¤ í™•ì¸\n",
    "    sample_texts = [dataset[i]['text'] for i in range(min(10, len(dataset)))]\n",
    "    max_indices = [text.max().item() for text in sample_texts]\n",
    "    print(f\"ì‹¤ì œ ìµœëŒ€ ì¸ë±ìŠ¤ë“¤: {max_indices}\")\n",
    "    print(f\"ëª¨ë“  ì¸ë±ìŠ¤ê°€ vocabulary ë²”ìœ„ ë‚´ì¸ê°€: {all(idx < vocab_size for idx in max_indices)}\")\n",
    "    \n",
    "    if any(idx >= vocab_size for idx in max_indices):\n",
    "        print(\"ì˜¤ë¥˜: ì¸ë±ìŠ¤ê°€ vocabulary ë²”ìœ„ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤!\")\n",
    "        return\n",
    "    \n",
    "    # 5. í•™ìŠµ ì‹œì‘\n",
    "    print(\"ğŸš€ Tacotron2 ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "    model, text_processor = train_tacotron2(\n",
    "        json_dir=json_dir,\n",
    "        audio_dir=audio_dir,\n",
    "        save_dir=save_dir,\n",
    "        epochs=10,\n",
    "        batch_size=1,  # 1ë¡œ ì¶•ì†Œ\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if model is None:\n",
    "        print(\"âŒ í•™ìŠµ ì‹¤íŒ¨!\")\n",
    "        return\n",
    "    \n",
    "    # 4. ëª¨ë¸ ê²½ëŸ‰í™”\n",
    "    print(\"âš¡ ëª¨ë¸ ê²½ëŸ‰í™” ì¤‘...\")\n",
    "    model = prune_model(model, amount=0.3)\n",
    "    quantized_model = quantize_model(model)\n",
    "    \n",
    "    # 5. ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "    final_save_path = os.path.join(save_dir, 'tacotron2_lightweight_final.pth')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': quantized_model.state_dict(),\n",
    "        'text_processor': text_processor\n",
    "    }, final_save_path)\n",
    "    print(f\"âœ… ê²½ëŸ‰í™”ëœ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_save_path}\")\n",
    "    \n",
    "    # 6. ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_text = \"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ ìŒì„±í•©ì„± í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.\"\n",
    "    \n",
    "    print(f\"ğŸ¤ ì¶”ë¡  í…ŒìŠ¤íŠ¸: '{test_text}'\")\n",
    "    mel_output = inference_tacotron2(quantized_model, text_processor, test_text, device)\n",
    "    \n",
    "    # 7. ì˜¤ë””ì˜¤ ìƒì„± ë° ì €ì¥\n",
    "    audio = mel_to_audio(mel_output)\n",
    "    \n",
    "    import soundfile as sf\n",
    "    output_audio_path = os.path.join(save_dir, 'output_test.wav')\n",
    "    sf.write(output_audio_path, audio, 22050)\n",
    "    print(f\"ğŸ”Š ìƒì„±ëœ ì˜¤ë””ì˜¤ ì €ì¥: {output_audio_path}\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ë¶„ì„ë§Œ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜\n",
    "def quick_analysis():\n",
    "    \"\"\"ë¹ ë¥¸ ë°ì´í„°ì…‹ ë¶„ì„\"\"\"\n",
    "    json_dir = r\"D:\\workspace\\TTS_prj\\datasets\\014.ë‹¤í™”ì ìŒì„±í•©ì„± ë°ì´í„°\\01.ë°ì´í„°\\1.Training\\ë¼ë²¨ë§ë°ì´í„°\\TL22\\TL22\\2.ì—¬ì„±\\2400ë¬¸ì¥\"\n",
    "    analyze_dataset_directory(json_dir)\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ë° ì‚¬ìš© í•¨ìˆ˜\n",
    "def load_and_use_model(model_path, text):\n",
    "    \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œí•˜ì—¬ ìŒì„± í•©ì„±\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    text_processor = checkpoint['text_processor']\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    vocab_size = len(text_processor.char_to_id)\n",
    "    model = Tacotron2(vocab_size).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # ì¶”ë¡ \n",
    "    mel_output = inference_tacotron2(model, text_processor, text, device)\n",
    "    audio = mel_to_audio(mel_output)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# JSON íŒŒì¼ êµ¬ì¡° í™•ì¸ í•¨ìˆ˜\n",
    "def check_json_structure(json_file):\n",
    "    \"\"\"JSON íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì—¬ TransLabelText ìœ„ì¹˜ íŒŒì•…\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def find_trans_label_text(obj, path=\"\"):\n",
    "        \"\"\"ì¬ê·€ì ìœ¼ë¡œ TransLabelText ì°¾ê¸°\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                current_path = f\"{path}.{key}\" if path else key\n",
    "                if key == \"TransLabelText\":\n",
    "                    print(f\"TransLabelText ë°œê²¬: {current_path} = {value}\")\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    find_trans_label_text(value, current_path)\n",
    "        elif isinstance(obj, list):\n",
    "            for i, item in enumerate(obj):\n",
    "                current_path = f\"{path}[{i}]\"\n",
    "                find_trans_label_text(item, current_path)\n",
    "    \n",
    "    print(f\"JSON íŒŒì¼ êµ¬ì¡° ë¶„ì„: {json_file}\")\n",
    "    find_trans_label_text(data)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì‹¤í–‰ ì˜µì…˜ ì„ íƒ\n",
    "    print(\"ğŸ¯ ì‹¤í–‰ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:\")\n",
    "    print(\"1. ë°ì´í„°ì…‹ ë¶„ì„ë§Œ ì‹¤í–‰\")\n",
    "    print(\"2. ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "    \n",
    "    choice = input(\"ì„ íƒ (1 ë˜ëŠ” 2): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        quick_analysis()\n",
    "    elif choice == \"2\":\n",
    "        main()\n",
    "    else:\n",
    "        print(\"ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
    "        quick_analysis()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
